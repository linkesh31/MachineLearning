# MachineLearning â€” Regression & Classification on Tabular Data

![Python](https://img.shields.io/badge/Python-3.10%2B-3776AB)
![scikit-learn](https://img.shields.io/badge/scikit--learn-1.x-FF9F1C)
![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-F37626)
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)

A compact, reproducible project that demonstrates **tabular ML** for two real-world problems:

- **Regression (Used-Car Price Prediction)** â€” predict `selling_price` from specs & usage.
- **Classification (Telecom Churn)** â€” predict whether a customer will churn.

Both notebooks use **sklearn pipelines** (impute â†’ scale/encode â†’ model), clear **metrics**, and tidy **plots**.  
Classification additionally shows **class-imbalance handling** with **SMOTE** and **model tuning**.

---

## ğŸ“¦ Repository Structure
MachineLearning/
â”œâ”€ regression.ipynb # Used-car price regression (RF vs MLP, tuning, learning curves)
â”œâ”€ classification.ipynb # Telco churn classification (RF vs MLP, SMOTE, tuning)
â”œâ”€ cars.csv # Regression dataset
â”œâ”€ churn.csv # Classification dataset
â””â”€ README.md


---

## âš™ï¸ Environment

```bash
# (optional) create & activate a virtual env
python -m venv .venv
# Windows
.venv\Scripts\activate
# macOS/Linux
# source .venv/bin/activate

# launch notebooks
jupyter notebook


pip install -U pip
pip install numpy pandas scikit-learn matplotlib seaborn imbalanced-learn jupyter
```
## ğŸš€ Quickstart

Open regression.ipynb or classification.ipynb.

Run all cells from top to bottom (datasets are included).

Inspect outputs: metrics, confusion matrices/ROC (classification), learning curves, and feature importance.

## ğŸ“˜ Whatâ€™s Inside the Notebooks
1) regression.ipynb â€” Used-Car Price

Data cleaning: parse units (e.g., "23.4 kmpl" â†’ 23.4), create mileage_num, engine_cc, max_power_bhp.

Outlier control: drop top 1% prices to stabilize fit.

Pipelines:

Numeric â†’ median impute + standardize

Categorical â†’ most-frequent impute + one-hot

Models: RandomForestRegressor vs MLPRegressor

Evaluation: MAE, MSE, RMSE, RÂ²

Extras: learning curves, MLP loss/val curves, feature importances, correlation heatmap.

Headline results (example run)

Tuned Random Forest: RMSE â‰ˆ 132k, RÂ² â‰ˆ 0.962

MLP: higher error on this tabular mix â†’ RF is the preferred baseline.

2) classification.ipynb â€” Telco Churn

Preprocessing: stratified split (Train/Val/Test), robust OHE, scaling, imputation.

Imbalance handling:

RF with class_weight="balanced"

MLP trained on SMOTE-oversampled Train split

Tuning:

RF: randomized search over trees/depth/splits/leaf/max_features (select by Val ROC-AUC)

MLP: grid of hidden sizes, alpha, learning_rate_init, batch_size (select by Val ROC-AUC)

Evaluation: Accuracy, Precision, Recall, F1, ROC-AUC, Confusion Matrix, ROC curve

Explainability: Random-Forest feature importances (top-k bar chart)

Headline results (example run)

RF (tuned): ROC-AUC â‰ˆ 0.84, balanced overall with good recall

MLP (tuned + SMOTE): ROC-AUC â‰ˆ 0.84 with higher minority-class recall/F1

Choose MLP when missing churners is costly; choose RF for slightly more balanced precision and simpler serving.

## ğŸ§© Key Ideas & Why They Matter

Pipelines prevent leakage; preprocessing is identical at train/test time.

SMOTE is applied only on Train to avoid inflating validation/test.

Validation-based tuning yields robust configs (not just train performance).

Feature importance (RF) provides quick explainability for stakeholders.

## ğŸ“Š Example Figures (auto-generated)

Regression: Actual vs Predicted, Residuals, Learning Curves, MLP loss/val curves, Feature Importances.

Classification: Confusion Matrices (Val/Test), ROC Curves (Val/Test), Top-k Feature Importances.

All images are generated when you run the notebooks; no external files required.

## âœ… Reproducibility Tips

Set random_state=42 (already used).

Run on Python 3.10+ and scikit-learn 1.x.

On Windows, a harmless warning about physical cores may appear; we cap n_jobs=1 where needed.

## ğŸ“ License

Released under the MIT License.

## ğŸ™Œ Acknowledgements

Datasets: educational examples for cars pricing and Telco churn.
Libraries: pandas, scikit-learn, imbalanced-learn, matplotlib, seaborn, Jupyter.
